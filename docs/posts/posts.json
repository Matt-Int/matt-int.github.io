[
  {
    "path": "posts/2020-11-28-cross-validation-with-r/",
    "title": "Cross-Validation with R",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Mattias",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nIn this document we will explore how to use cross-validation as a tool for model selection and what some of the best-practices are in terms of estimating model fitness.\nThis document should accompany a presentation based on the same code, although for brevity the presentation has been simplified and may not contain all the commentary available on here.\nIf you wanted to run the code in this document then you would have to install and load the following packages: - tidyverse - rsample - ranger\nThe data\nFirst we should have a quick look at the data we will be creating our model for. We will be using the diamonds dataset from the ggplot2 package.\nA quick look\nFrom glimpseing the data we can see that it is made up of 10 features across 53940 observations. Additionally, each feature is encoded appropriately as ordered factors (ordinal) or as doubles (numerical). As this data is already preprocessed we will not be doing any data-manipulation to pre-process the data.\nIf we were doing some kind of cluster analysis as opposed to predictive modelling then we would perhaps consider centering and scaling our features, however, as we are primarily looking to only test linear models and random forests, no scaling should be necessary.\n\n\nglimpse(diamonds)\n\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.2…\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Ver…\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, …\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1,…\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.…\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61,…\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 3…\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.8…\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.7…\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.4…\n\nThe modelling\nMethodology\nFor simplicity we will use all 10 available features to try and predict the price of a diamond. We will also use cross-validation to compare two kinds of models prior to testing our final model’s accuracy.\nIn essence we will go through the following steps:\nSplit the data into a training and testing set\nCreate a 5-fold cross-validation set out of the testing set\nTrain a model on the training data using a linear model and random forest\nCompare the RMSE of both models to find which performs best\nSelect the best performing model\nTrain on the full training data\nFind the accuracy with the new model by comparing predicted prices vs. actual prices in the test data\nPreparing the data\nFor reproducibility we will set a seed at the start of each code chunk. This is purely so that if you were to rerun this code you should get the exact same results as in this document.\n\n\nset.seed(1126)\ndata_split <- initial_split(diamonds, prop = 0.75)\ntraining_data <- training(data_split)\ntesting_data <- testing(data_split)\n\n\n\nThis will split the data into two sets, a training set and a testing set. As we specified the prop parameter to be equal to 0.75 we can expect around 75% of the data to be in the training set and 25% in the test set.\nWhat is cross-validation?\nA simplified version of cross-validation could be described like so:\nSplit the data into v splits (or folds)\nPartition each fold into a training and validation set\nTrain a model and validate its performance\nThe average error is then used to estimate the performance of this type of model\nCreating cross-validations\nThe package we will use for cross-validation in R is called rsample.\n\n\nset.seed(1126)\ncv_split <- vfold_cv(training_data, v = 5)\ncv_data <- cv_split %>%\n  mutate(\n         train = map(splits, ~training(.x)),\n         validate = map(splits, ~testing(.x))\n  )\ncv_data\n\n\n#  5-fold cross-validation \n# A tibble: 5 x 4\n  splits               id    train                validate            \n  <list>               <chr> <list>               <list>              \n1 <split [32.4K/8.1K]> Fold1 <tibble [32,364 × 1… <tibble [8,091 × 10…\n2 <split [32.4K/8.1K]> Fold2 <tibble [32,364 × 1… <tibble [8,091 × 10…\n3 <split [32.4K/8.1K]> Fold3 <tibble [32,364 × 1… <tibble [8,091 × 10…\n4 <split [32.4K/8.1K]> Fold4 <tibble [32,364 × 1… <tibble [8,091 × 10…\n5 <split [32.4K/8.1K]> Fold5 <tibble [32,364 × 1… <tibble [8,091 × 10…\n\nThe way the code above works is that it calls the vfold_cv function and passes the training data to it. We then specify the v argument to be 5, this is the number of folds we want to use.\nWe store the result of this function call as a variable called cv_data (crossvalidation_data). We then take cv_data and mutate it to create two new variables (columns); train and validate.\nThe contents of these two will be the splits column, which is part of cv_data, and splitting that out to a train and testing combo. In the case of cross-validation however, we call our test set “validation” instead.\nMeasuring the accuracy of a model\nAs we want to pick the best model in terms of predictive power we need a way to estimate how good different models perform in our cross-validation tests. One way to do so is to use the Root Mean Square Error, also known as RMSE. This is a measure of how much, on average, our model is wrong. It is a common measure for determining the accuracy of regression models. If you were building a classification model instead then you would probably want to use another accuracy measure like accuracy, recall, or precision which are implemented in the Metrics package.\nNote: we will not cover these in this document but may be part of a future training\nThe RMSE can be calculated as follows:\n\\[ \\sqrt{\\frac{\\sum_{i = 1}^{n} (\\hat{y_{i}} - y_{i})^2}{n}} \\]\nWhere \\(\\hat{y}\\) is the predicted value and \\(y\\) is the observed value.\nUsing map to fit linear models to predict price\nWe can again use map and its sibling map2 to:\nFit a model to each training set,\nPredict what the price should be for each observation in the validation set,\nExtract what the actual value was for each observation in the validation set\nCalculate the RMSE through subtracting all the predicted values from the observed values, squaring the difference, taking the mean of them and squaring the result.\nAfter these steps we end up with a data frame that has the RMSE for each fold, we can then summarise these together to see what the average RMSE was for our linear model.\n\n\nset.seed(1126)\nrmse_lm <- cv_data %>%\n  mutate(\n         model = map(train, ~lm(price ~ ., data = .x)),\n         price_predicted = map2(model, validate, ~predict(.x, .y)),\n         price_actual = map(validate, ~.x$price),\n         rmse = map2_dbl(price_predicted, price_actual, ~sqrt(mean((.x - .y)^2)))\n         ) %>%\n  summarise(rmse = mean(rmse)) %>% pull()\n\nrmse_lm\n\n\n[1] 1166.894\n\nThat gives us an RMSE of 1167, that means that, on average, this model is around $1166.89 off.\nLet’s compare it with a random forest\nWe can run the exact same modelling process but instead of lm we can fit a random forest using the ranger package. The primary difference in the two (apart from the model function) is that for the predict function we have to specify the prediction element from the returned object. This is specified by the “$prediction” part.\n\n\nset.seed(1126)\nrmse_ranger <- cv_data %>%\n  mutate(\n         model = map(train, ~ranger(price ~ ., data = .x)),\n         price_predicted = map2(model, validate, ~predict(.x,.y)$prediction),\n         price_actual = map(validate, ~.x$price),\n         rmse = map2_dbl(price_predicted, price_actual, ~sqrt(mean((.x-.y)^2)))\n  ) %>%\n  summarise(rmse = mean(rmse)) %>% pull()\nrmse_ranger\n\n\n[1] 561.913\n\nThis seems to be a better fit to our data as the random forest is on average $605 closer to the observed price.\nSelecting a model\nAs such we will select the Random forest model as it has a smaller error compared to the linear model. However, it might be prudent to see which random forest will perform the best as tweaking some parameters could improve accuracy.\nFor this example we will tweak the mtry parameter as it could have a positive (or negative) impact on our model’s predictive power.\n\n\nset.seed(1126)\ncv_data %>%\n  crossing(mtry = 1:6) %>%\n  mutate(\n         model = map2(train, mtry, ~ranger(price ~ ., data = .x, mtry = .y)),\n         price_predicted = map2(model, validate, ~predict(.x,.y)$prediction),\n         price_actual = map(validate, ~.x$price),\n         rmse = map2_dbl(price_predicted, price_actual, ~sqrt(mean((.x-.y)^2)))\n  ) %>%\n  group_by(mtry) %>%\n  summarise(rmse = mean(rmse)) \n\n\n# A tibble: 6 x 2\n   mtry  rmse\n  <int> <dbl>\n1     1  789.\n2     2  598.\n3     3  562.\n4     4  550.\n5     5  547.\n6     6  547.\n\nThe main difference from our previous cross-validation is that we have used the crossing function to expand the data with a new variable, essentially copying the original cv_data into 6 version with the new mtry variable the only difference.\nWe can then use map2 to pass the mtry variable in and fit random forests with these different ones.\nNote: If you were doing some cluster analysis, this would be a good way to run multiple versions of kmeans to then do a silhouette analysis or similar.\nWe can see the default mtry = 3 performs better than 1 and 2 but we seem to reduce the error more at mtry = 5\nQuick recap\nWe have so far compared a random forest and a linear model and selected the random forest as it’s prediction error was lower than for the linear model.\nWe then compared the accuracy of different random forests with different parameters.\nOur final model is a random forest with the parameter mtry = 5\nRunning and testing the accuracy of our model\nLet’s fit our random forest, with mtry = 5 as our parameter on our full training_data that we defined at the beginning. We can then generate the predicted prices from our testing_data and extract the actual prices from the same set.\n\n\nset.seed(1126)\nmodel <- ranger(price ~ ., data = training_data, mtry = 5)\nprice_predicted  <- predict(model, testing_data)$predict\nprice_actual <- testing_data$price\n\n(rmse <- sqrt(mean((price_predicted - price_actual)^2)))\n\n\n[1] 536.9415\n\nCalculating the RMSE shows us an average error of approx. $537.\nVisualising the performance of our model\nFinally we can quickly visualise the performance of our model by plotting the predicted values against the actual values in a scatter plot. We can also add a line with a slope of 1 and an intercept of 0 to show where the points should be if there was no error in our model.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2020-11-28T14:33:51+00:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Data Blogging",
    "description": "Welcome to our new blog, Data Blogging. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2020-11-28T14:23:26+00:00",
    "input_file": {}
  }
]
